{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12346d68-c5bc-481e-9217-857ca58198ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e179523-6ba4-42e9-9da9-045dcf6d62a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "A=\"\"\"    def train_test(self,batch_size,epochs=1,mode=1,early_stopping_threshold=3): #1=train,2=test,3=eval\n",
    "\n",
    "        self.Model.eval()\n",
    "        for param in self.Model.parameters():\n",
    "            param.requires_grad = False  # This freezes the weights\n",
    "\n",
    "        if mode==1:\n",
    "            self.Transformation_Class.phi.train()\n",
    "            for param in self.Transformation_Class.phi.parameters():\n",
    "                param.requires_grad = True  # This unfreezes the weights\n",
    "            data=self.Train_Dataset\n",
    "            Sample_Indices=list(range(self.Train_Sample_Number))\n",
    "        elif mode==2:\n",
    "            self.Transformation_Class.phi.eval()\n",
    "            for param in self.Transformation_Class.phi.parameters():\n",
    "                param.requires_grad = False  # This freezes the weights\n",
    "            data=self.Test_Dataset\n",
    "            Sample_Indices=list(range(self.Test_Samples_Number))\n",
    "        elif mode==3:\n",
    "            self.Transformation_Class.phi.eval()\n",
    "            for param in self.Transformation_Class.phi.parameters():\n",
    "                param.requires_grad = False  # This freezes the weights\n",
    "            data=self.Eval_Dataset\n",
    "            Sample_Indices=list(range(self.Eval_Samples_Number))\n",
    "        \n",
    "        best_accuracy=-1\n",
    "        steps_without_improvement=0\n",
    "        Best_Phi=None\n",
    "        for epoch in range(epochs):\n",
    "            total_correct = 0\n",
    "            total_samples = 0\n",
    "            total_loss = 0\n",
    "        \n",
    "            #Make Batches\n",
    "            random.shuffle(Sample_Indices)\n",
    "            DAS_Train_Batches=self.chunk_list(Sample_Indices, batch_size)\n",
    "\n",
    "            \n",
    "            for ac_batch in tqdm(DAS_Train_Batches):\n",
    "\n",
    "                if mode==1: \n",
    "                    self.Transformation_Class.optimizer.zero_grad()\n",
    "                    \n",
    "                loss,total_correct,total_samples = self.process_Batch(mode,data,ac_batch,total_correct,total_samples)\n",
    "                \n",
    "                if mode==1:\n",
    "                    loss.backward()\n",
    "                    self.Transformation_Class.optimizer.step()\n",
    "                    total_loss += loss.item()\n",
    "            \n",
    "            if mode==1:\n",
    "                eval_acc=self.train_test(batch_size,epochs=1,mode=3)\n",
    "                if eval_acc>best_accuracy:\n",
    "                    best_accuracy=eval_acc\n",
    "                    steps_without_improvement=0\n",
    "                    Best_Phi=copy.deepcopy(self.Transformation_Class.phi)\n",
    "                else:\n",
    "                    steps_without_improvement+=1\n",
    "                if steps_without_improvement>=early_stopping_threshold or epoch==epochs-1:\n",
    "                    Best_Phi.to(self.Device)\n",
    "                    Best_Phi_inverse=InverseTransformation_Function(Best_Phi)\n",
    "                    optimizer = optim.Adam(Best_Phi.parameters(), lr=0.001)\n",
    "                    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=10)\n",
    "                    self.Transformation_Class=phi_class(Best_Phi,Best_Phi_inverse,LLM_Criterion,optimizer,scheduler)\n",
    "                    break\n",
    "                total_loss=total_loss / len(DAS_Train_Batches)\n",
    "                self.Transformation_Class.scheduler.step(total_loss)\n",
    "                print(f\"Epoch {epoch+1}, Loss: {total_loss}\",\n",
    "                      \"steps without improvement:\",steps_without_improvement,\n",
    "                      \"best accuracy:\",best_accuracy,\n",
    "                      \"learning rate:\",self.Transformation_Class.scheduler.get_last_lr())\n",
    "                \n",
    "        if mode==2 or mode==3: \n",
    "            self.Transformation_Class.phi.train()\n",
    "            for param in self.Transformation_Class.phi.parameters():\n",
    "                param.requires_grad = True  # This unfreezes the weights\n",
    "            accuracy = total_correct / total_samples\n",
    "            return accuracy\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "961ae313-96c1-4bcf-a0a9-9b674fcca460",
   "metadata": {},
   "outputs": [],
   "source": [
    "B=\"\"\"    def train_test(self,batch_size,epochs=1,mode=1,early_stopping_threshold=3): #1=train,2=test,3=eval\n",
    "\n",
    "        self.Model.eval()\n",
    "        for param in self.Model.parameters():\n",
    "            param.requires_grad = False  # This freezes the weights\n",
    "\n",
    "        if mode==1:\n",
    "            self.Transformation_Class.phi.train()\n",
    "            for param in self.Transformation_Class.phi.parameters():\n",
    "                param.requires_grad = True  # This unfreezes the weights\n",
    "            data=self.Train_Dataset\n",
    "            Sample_Indices=list(range(self.Train_Sample_Number))\n",
    "        elif mode==2:\n",
    "            self.Transformation_Class.phi.eval()\n",
    "            for param in self.Transformation_Class.phi.parameters():\n",
    "                param.requires_grad = False  # This freezes the weights\n",
    "            data=self.Test_Dataset\n",
    "            Sample_Indices=list(range(self.Test_Samples_Number))\n",
    "        elif mode==3:\n",
    "            self.Transformation_Class.phi.eval()\n",
    "            for param in self.Transformation_Class.phi.parameters():\n",
    "                param.requires_grad = False  # This freezes the weights\n",
    "            data=self.Eval_Dataset\n",
    "            Sample_Indices=list(range(self.Eval_Samples_Number))\n",
    "        \n",
    "        best_accuracy=-1\n",
    "        steps_without_improvement=0\n",
    "        Best_Phi=None\n",
    "        for epoch in range(epochs):\n",
    "            total_correct = 0\n",
    "            total_samples = 0\n",
    "            total_loss = 0\n",
    "        \n",
    "            #Make Batches\n",
    "            random.shuffle(Sample_Indices)\n",
    "            DAS_Train_Batches=self.chunk_list(Sample_Indices, batch_size)\n",
    "            \n",
    "            for ac_batch in DAS_Train_Batches:\n",
    "\n",
    "                if mode==1: \n",
    "                    self.Transformation_Class.optimizer.zero_grad()\n",
    "                \n",
    "                \n",
    "                loss,total_correct,total_samples = self.process_Batch(self,mode,data,ac_batch,total_correct,total_samples)\n",
    "                \n",
    "                if mode==1:\n",
    "                    loss.backward()\n",
    "                    self.Transformation_Class.optimizer.step()\n",
    "                    total_loss += loss.item()\n",
    "\n",
    "            if mode==1:\n",
    "                eval_acc=self.train_test(batch_size,epochs=1,mode=3)\n",
    "                if eval_acc>best_accuracy:\n",
    "                    best_accuracy=eval_acc\n",
    "                    steps_without_improvement=0\n",
    "                    Best_Phi=copy.deepcopy(self.Transformation_Class.phi)\n",
    "                else:\n",
    "                    steps_without_improvement+=1\n",
    "                if steps_without_improvement>=early_stopping_threshold or epoch==epochs-1:\n",
    "                    Best_Phi.to(self.Device)\n",
    "                    Best_Phi_inverse=Best_Phi.inverse\n",
    "                    criterion = nn.CrossEntropyLoss()\n",
    "                    optimizer = optim.Adam(Best_Phi.parameters(), lr=0.001)\n",
    "                    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=10)\n",
    "                    self.Transformation_Class=phi_class(Best_Phi,Best_Phi_inverse,criterion,optimizer,scheduler)\n",
    "                    break\n",
    "                total_loss=total_loss / len(DAS_Train_Batches)\n",
    "                self.Transformation_Class.scheduler.step(total_loss)\n",
    "                print(f\"Epoch {epoch+1}, Loss: {total_loss}\",\n",
    "                      \"steps without improvement:\",steps_without_improvement,\n",
    "                      \"best accuracy:\",best_accuracy,\n",
    "                      \"learning rate:\",self.Transformation_Class.scheduler.get_last_lr())\n",
    "                \n",
    "        if mode==2 or mode==3: \n",
    "            self.Transformation_Class.phi.train()\n",
    "            for param in self.Transformation_Class.phi.parameters():\n",
    "                param.requires_grad = True  # This unfreezes the weights\n",
    "            accuracy = total_correct / total_samples\n",
    "            return accuracy\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c67e0f4-6983-431a-82d9-2108994a31ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['      def train_test(self,batch_size,epochs=1,mode=1,early_stopping_threshold=3): #1=train,2=test,3=eval\\n',\n",
       " '  \\n',\n",
       " '          self.Model.eval()\\n',\n",
       " '          for param in self.Model.parameters():\\n',\n",
       " '              param.requires_grad = False  # This freezes the weights\\n',\n",
       " '  \\n',\n",
       " '          if mode==1:\\n',\n",
       " '              self.Transformation_Class.phi.train()\\n',\n",
       " '              for param in self.Transformation_Class.phi.parameters():\\n',\n",
       " '                  param.requires_grad = True  # This unfreezes the weights\\n',\n",
       " '              data=self.Train_Dataset\\n',\n",
       " '              Sample_Indices=list(range(self.Train_Sample_Number))\\n',\n",
       " '          elif mode==2:\\n',\n",
       " '              self.Transformation_Class.phi.eval()\\n',\n",
       " '              for param in self.Transformation_Class.phi.parameters():\\n',\n",
       " '                  param.requires_grad = False  # This freezes the weights\\n',\n",
       " '              data=self.Test_Dataset\\n',\n",
       " '              Sample_Indices=list(range(self.Test_Samples_Number))\\n',\n",
       " '          elif mode==3:\\n',\n",
       " '              self.Transformation_Class.phi.eval()\\n',\n",
       " '              for param in self.Transformation_Class.phi.parameters():\\n',\n",
       " '                  param.requires_grad = False  # This freezes the weights\\n',\n",
       " '              data=self.Eval_Dataset\\n',\n",
       " '              Sample_Indices=list(range(self.Eval_Samples_Number))\\n',\n",
       " '          \\n',\n",
       " '          best_accuracy=-1\\n',\n",
       " '          steps_without_improvement=0\\n',\n",
       " '          Best_Phi=None\\n',\n",
       " '          for epoch in range(epochs):\\n',\n",
       " '              total_correct = 0\\n',\n",
       " '              total_samples = 0\\n',\n",
       " '              total_loss = 0\\n',\n",
       " '          \\n',\n",
       " '              #Make Batches\\n',\n",
       " '              random.shuffle(Sample_Indices)\\n',\n",
       " '              DAS_Train_Batches=self.chunk_list(Sample_Indices, batch_size)\\n',\n",
       " '- \\n',\n",
       " '              \\n',\n",
       " '-             for ac_batch in tqdm(DAS_Train_Batches):\\n',\n",
       " '?                             -----                 -\\n',\n",
       " '+             for ac_batch in DAS_Train_Batches:\\n',\n",
       " '  \\n',\n",
       " '                  if mode==1: \\n',\n",
       " '                      self.Transformation_Class.optimizer.zero_grad()\\n',\n",
       " '-                     \\n',\n",
       " '? ----\\n',\n",
       " '+                 \\n',\n",
       " '+                 \\n',\n",
       " '-                 loss,total_correct,total_samples = self.process_Batch(mode,data,ac_batch,total_correct,total_samples)\\n',\n",
       " '+                 loss,total_correct,total_samples = self.process_Batch(self,mode,data,ac_batch,total_correct,total_samples)\\n',\n",
       " '?                                                                       +++++\\n',\n",
       " '                  \\n',\n",
       " '                  if mode==1:\\n',\n",
       " '                      loss.backward()\\n',\n",
       " '                      self.Transformation_Class.optimizer.step()\\n',\n",
       " '                      total_loss += loss.item()\\n',\n",
       " '-             \\n',\n",
       " '+ \\n',\n",
       " '              if mode==1:\\n',\n",
       " '                  eval_acc=self.train_test(batch_size,epochs=1,mode=3)\\n',\n",
       " '                  if eval_acc>best_accuracy:\\n',\n",
       " '                      best_accuracy=eval_acc\\n',\n",
       " '                      steps_without_improvement=0\\n',\n",
       " '                      Best_Phi=copy.deepcopy(self.Transformation_Class.phi)\\n',\n",
       " '                  else:\\n',\n",
       " '                      steps_without_improvement+=1\\n',\n",
       " '                  if steps_without_improvement>=early_stopping_threshold or epoch==epochs-1:\\n',\n",
       " '                      Best_Phi.to(self.Device)\\n',\n",
       " '-                     Best_Phi_inverse=InverseTransformation_Function(Best_Phi)\\n',\n",
       " '+                     Best_Phi_inverse=Best_Phi.inverse\\n',\n",
       " '+                     criterion = nn.CrossEntropyLoss()\\n',\n",
       " '                      optimizer = optim.Adam(Best_Phi.parameters(), lr=0.001)\\n',\n",
       " '                      scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=10)\\n',\n",
       " '-                     self.Transformation_Class=phi_class(Best_Phi,Best_Phi_inverse,LLM_Criterion,optimizer,scheduler)\\n',\n",
       " '?                                                                                   ^^^^^\\n',\n",
       " '+                     self.Transformation_Class=phi_class(Best_Phi,Best_Phi_inverse,criterion,optimizer,scheduler)\\n',\n",
       " '?                                                                                   ^\\n',\n",
       " '                      break\\n',\n",
       " '                  total_loss=total_loss / len(DAS_Train_Batches)\\n',\n",
       " '                  self.Transformation_Class.scheduler.step(total_loss)\\n',\n",
       " '                  print(f\"Epoch {epoch+1}, Loss: {total_loss}\",\\n',\n",
       " '                        \"steps without improvement:\",steps_without_improvement,\\n',\n",
       " '                        \"best accuracy:\",best_accuracy,\\n',\n",
       " '                        \"learning rate:\",self.Transformation_Class.scheduler.get_last_lr())\\n',\n",
       " '                  \\n',\n",
       " '          if mode==2 or mode==3: \\n',\n",
       " '              self.Transformation_Class.phi.train()\\n',\n",
       " '              for param in self.Transformation_Class.phi.parameters():\\n',\n",
       " '                  param.requires_grad = True  # This unfreezes the weights\\n',\n",
       " '              accuracy = total_correct / total_samples\\n',\n",
       " '              return accuracy']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matcher = difflib.Differ()\n",
    "list(matcher.compare(a=A.splitlines(keepends=True), b=B.splitlines(keepends=True)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
