{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f9c7a74-0fa0-4744-819a-12619cec2503",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import copy\n",
    "import json\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b227db55-31c7-4a24-8c73-b6aa17893a34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/dummy/dummy/runs/z6rzue01?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7716df8b9d00>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init(mode=\"disabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "288d0ed6-20a0-45d1-b3a3-e0f9dea87b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from das.Classification_Model import (MLPForClassification,\n",
    "                                  train_model,\n",
    "                                  eval_model,\n",
    "                                  test_model,\n",
    "                                  make_model)\n",
    "from das.Helper_Functions import set_seed\n",
    "from das.Dataset_Generation import (make_model_dataset,\n",
    "                                make_model_dataset_AndOrAnd,\n",
    "                                make_intervention_dataset_variable_intervention_all,\n",
    "                                make_intervention_dataset_variable_intervention_first,\n",
    "                                make_intervention_dataset_first_input_intervention,\n",
    "                                make_intervention_dataset_AndOrAnd,\n",
    "                                make_intervention_dataset_AndOr)\n",
    "from das.RevNet import RevNet\n",
    "from das.Rotation_Model import Rotation\n",
    "from das.DAS import phi_class\n",
    "from das.DAS_MLP import Distributed_Alignment_Search_MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69c97efd-716b-4155-a0c7-85ffa6295edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE  = \"cpu\" #\"cuda\"/\"cpu\"\n",
    "num_classes=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "239df68b-bb77-4ad5-b757-b38931675f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_intervention_hook(Save_array,Pos,layer):\n",
    "    def hook_fn(module, input, output):\n",
    "        Save_array[Pos].append(output.detach().cpu())\n",
    "    layer.register_forward_hook(hook_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53e98d55-7e8a-4dec-872c-7b6b1ca6a144",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/master/DAS/Complexity-vs.-Accuracy/notebooks/../das/Dataset_Generation.py:64: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
      "  return torch.tensor(model_inputs, dtype=torch.float32).to(device),torch.tensor(labels, dtype=torch.float32).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.39049801870714873 steps without improvement: 7 best accuracy: 0.9837\n",
      "Epoch 2, Loss: 0.026408711896237946 steps without improvement: 72 best accuracy: 0.9964\n",
      "Epoch 3, Loss: 0.008641325565577063 steps without improvement: 37 best accuracy: 0.9986\n",
      "Epoch 4, Loss: 0.004503298025667846 steps without improvement: 60 best accuracy: 0.9991\n",
      "Epoch 5, Loss: 0.0028202552880998155 steps without improvement: 138 best accuracy: 0.9997\n",
      "Epoch 6, Loss: 0.0019203961838059058 steps without improvement: 301 best accuracy: 0.9998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5000/5000 [45:42<00:00,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surjectivity: Missing: set() Found: {0, 1}\n",
      "Injectivity [[1.6054958673748023, 0.0, 1.6054210510295241, 1.6019198240301147], [1.117766764482549, 0.0, 1.0932176194103445, 0.9145127258176602], [1.9995332732080362, 0.0, 1.9030948216695343, 1.280946735286285], [4.2082161881334255, 0.0, 3.2319988631395575, 2.1088286168429353]]\n",
      "Injectivity [[12497500, 20064684.60251659, 0, 6247669, 10030139.332464576, 3123817, 5004104.378942281], [12497500, 13969290.139120657, 0, 6247669, 6830061.831043807, 3123817, 2856770.399625546], [12497500, 24989167.08191743, 0, 6247669, 11889906.521405278, 3123817, 4001443.1877817973], [12497500, 52592181.81119748, 0, 6247669, 20192459.105272256, 3123817, 6587594.683380447]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results=[]\n",
    "for acseed in [4287]:#, 3837, 9097, 2635, 5137, 6442, 5234, 4641, 8039, 2266]:\n",
    "    results.append({})\n",
    "    set_seed(acseed)\n",
    "    X_train,y_train = make_model_dataset(1048576,4,DEVICE)#1048576\n",
    "    X_eval,y_eval   = make_model_dataset(10000,4,DEVICE)#10000\n",
    "    X_test,y_test   = make_model_dataset(10000,4,DEVICE)\n",
    "    X_inj,y_inj   = make_model_dataset(5000,4,DEVICE)#5000\n",
    "   \n",
    "    model,accuracy=make_model(X_train,y_train,X_eval,y_eval,X_test,y_test,input_size=16,epochs=20,device=DEVICE)\n",
    "    Layers=[]\n",
    "    Layers.append((\"Layer1\",model.mlp.h[0]))\n",
    "    Layers.append((\"Layer2\",model.mlp.h[1]))\n",
    "    Layers.append((\"Layer3\",model.mlp.h[2]))\n",
    "\n",
    "\n",
    "    Layer_Save=[[],[],[],[]]\n",
    "    Results=[]\n",
    "    Results.append([0,0,0,0,0,0,0])\n",
    "    Hooks=[]\n",
    "    predicted_classes_set = set()\n",
    "    for acpos,aclayer in enumerate(Layers):\n",
    "        Layer_Save.append([])\n",
    "        Results.append([0,0,0,0,0,0,0])\n",
    "        Hooks.append(register_intervention_hook(Layer_Save,acpos+1,aclayer[1]))\n",
    "    \n",
    "    \n",
    "    test_dataset = TensorDataset(X_inj,y_inj)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=5, shuffle=False)#6400\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            Layer_Save[0].append(X_batch)\n",
    "            output=model(X_batch)\n",
    "            predicted = torch.argmax(output, dim=1)\n",
    "            Layer_Save[-3].append(predicted)\n",
    "            Layer_Save[-2].append(torch.all(X_batch[:, :4] == X_batch[:, 4:8], dim=1))\n",
    "            Layer_Save[-1].append(torch.all(X_batch[:, 8:12] == X_batch[:, 12:16], dim=1))\n",
    "        \n",
    "            # Update set with predictions from this batch\n",
    "            predicted_classes_set.update(predicted.tolist())\n",
    "    for i in range(len(Layer_Save)):\n",
    "        Layer_Save[i]=torch.cat(Layer_Save[i])\n",
    "    for i in tqdm(range(len(Layer_Save[0]))):\n",
    "        for j in range(i+1,len(Layer_Save[0])):\n",
    "            Results[0][0]+=1\n",
    "            Results[0][1]+=torch.norm(Layer_Save[0][i]-Layer_Save[0][j], p=2).item()\n",
    "            if Layer_Save[-3][i]==Layer_Save[-3][j]:\n",
    "                Results[0][3]+=1\n",
    "                Results[0][4]+=torch.norm(Layer_Save[0][i]-Layer_Save[0][j], p=2).item()\n",
    "            if Layer_Save[-2][i]==Layer_Save[-2][j] and Layer_Save[-1][i]==Layer_Save[-1][j]:\n",
    "                Results[0][5]+=1\n",
    "                Results[0][6]+=torch.norm(Layer_Save[0][i]-Layer_Save[0][j], p=2).item()\n",
    "            if torch.equal(Layer_Save[0][i],Layer_Save[0][j]):\n",
    "                Results[0][2]+=1\n",
    "            for k in range(1,len(Layer_Save)-3):\n",
    "                Results[k][0]+=1\n",
    "                Results[k][1]+=torch.norm(Layer_Save[k][i]-Layer_Save[k][j], p=2).item()\n",
    "                if torch.equal(Layer_Save[k][i],Layer_Save[k][j]):\n",
    "                    Results[k][2]+=1\n",
    "                if Layer_Save[-3][i]==Layer_Save[-3][j]:\n",
    "                    Results[k][3]+=1\n",
    "                    Results[k][4]+=torch.norm(Layer_Save[k][i]-Layer_Save[k][j], p=2).item()\n",
    "                if Layer_Save[-2][i]==Layer_Save[-2][j] and Layer_Save[-1][i]==Layer_Save[-1][j]:\n",
    "                    Results[k][5]+=1\n",
    "                    Results[k][6]+=torch.norm(Layer_Save[k][i]-Layer_Save[k][j], p=2).item()\n",
    "    \n",
    "    all_classes_set = set(range(num_classes))\n",
    "    missing_classes_set = all_classes_set - predicted_classes_set\n",
    "    Results_processed=[]\n",
    "    for i in Results:\n",
    "        Results_processed.append([i[1]/i[0],i[2]/i[0],i[4]/i[3],i[6]/i[5]])\n",
    "    print(\"Surjectivity: Missing:\", missing_classes_set,\"Found:\",predicted_classes_set)\n",
    "    print(\"Injectivity\", Results_processed)  \n",
    "    print(\"Injectivity\", Results)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a91fe001-0b07-4f50-b82e-e415f31cec68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[12497500,\n",
       "  20064684.60251659,\n",
       "  0,\n",
       "  6247669,\n",
       "  10030139.332464576,\n",
       "  3123817,\n",
       "  5004104.378942281],\n",
       " [12497500,\n",
       "  13969290.139120657,\n",
       "  0,\n",
       "  6247669,\n",
       "  6830061.831043807,\n",
       "  3123817,\n",
       "  2856770.399625546],\n",
       " [12497500,\n",
       "  24989167.08191743,\n",
       "  0,\n",
       "  6247669,\n",
       "  11889906.521405278,\n",
       "  3123817,\n",
       "  4001443.1877817973],\n",
       " [12497500,\n",
       "  52592181.81119748,\n",
       "  0,\n",
       "  6247669,\n",
       "  20192459.105272256,\n",
       "  3123817,\n",
       "  6587594.683380447]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c2d7e98-1229-48d2-afa1-1b6ed916b02e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.6054958673748023, 0.0, 1.6054210510295241, 1.6019198240301147],\n",
       " [1.117766764482549, 0.0, 1.0932176194103445, 0.9145127258176602],\n",
       " [1.9995332732080362, 0.0, 1.9030948216695343, 1.280946735286285],\n",
       " [4.2082161881334255, 0.0, 3.2319988631395575, 2.1088286168429353]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Results_processed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
