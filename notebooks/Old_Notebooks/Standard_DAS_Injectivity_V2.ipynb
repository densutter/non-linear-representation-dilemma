{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9034afed-d664-4d77-943e-e9275fd35689",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import copy\n",
    "import json\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "854478d9-c914-4e17-8b1a-6838edc1940b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from das.Classification_Model import (MLPForClassification,\n",
    "                                  train_model,\n",
    "                                  eval_model,\n",
    "                                  test_model,\n",
    "                                  make_model)\n",
    "from das.Helper_Functions import *\n",
    "from das.Dataset_Generation import (make_model_dataset,\n",
    "                                make_model_dataset_AndOrAnd,\n",
    "                                make_intervention_dataset_variable_intervention_all,\n",
    "                                make_intervention_dataset_variable_intervention_first,\n",
    "                                make_intervention_dataset_first_input_intervention,\n",
    "                                make_intervention_dataset_AndOrAnd,\n",
    "                                make_intervention_dataset_AndOr)\n",
    "from das.RevNet import RevNet\n",
    "from das.Rotation_Model import Rotation\n",
    "from das.DAS import phi_class\n",
    "from das.DAS_MLP import Distributed_Alignment_Search_MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4791cbfe-2f92-40b2-a05b-18acabc98d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def analyze_layer_distances(Layer_Save, sample_size=1000, device='cuda'):\n",
    "    # Convert to torch tensor\n",
    "    feature_layers = torch.stack([torch.tensor(arr) for arr in Layer_Save[:-3]]).to(device)  # [num_layers, num_points, dim]\n",
    "    os_labels = torch.tensor(Layer_Save[-3]).to(device)\n",
    "    vs_labels = torch.stack([torch.tensor(Layer_Save[-2]), torch.tensor(Layer_Save[-1])], dim=1).to(device)\n",
    "\n",
    "    num_layers, num_points, dim = feature_layers.shape\n",
    "\n",
    "    # Sample indices\n",
    "    all_indices = list(range(num_points))\n",
    "    sampled = random.sample(all_indices, sample_size)\n",
    "    sampled_tensor = torch.tensor(sampled, device=device)\n",
    "\n",
    "    # Initialize result structure\n",
    "    categories = [\"AP\", \"OS\", \"VS\", \"nOS\", \"nVS\"]\n",
    "    Results = [\n",
    "        {key: torch.tensor([0, 0.0, float('inf')], device=device) for key in categories}\n",
    "        for _ in range(num_layers)\n",
    "    ]\n",
    "\n",
    "    for i_h, i in enumerate(tqdm(sampled_tensor.tolist(), desc=\"Processing samples\")):\n",
    "        exclude_mask = torch.zeros(num_points, dtype=torch.bool, device=device)\n",
    "        exclude_mask[sampled_tensor[:i_h+1]] = True\n",
    "\n",
    "        for k in range(num_layers):\n",
    "            xi = feature_layers[k, i]  # [dim]\n",
    "            xj = feature_layers[k]     # [num_points, dim]\n",
    "\n",
    "            # Euclidean distances (broadcasted)\n",
    "            dists = torch.norm(xj - xi, dim=1)  # [num_points]\n",
    "            dists = dists.masked_fill(exclude_mask, float('inf'))\n",
    "\n",
    "            # Label masks\n",
    "            os_mask = (os_labels == os_labels[i]) & ~exclude_mask\n",
    "            vs_mask = ((vs_labels[:, 0] == vs_labels[i, 0]) &\n",
    "                       (vs_labels[:, 1] == vs_labels[i, 1]) &\n",
    "                       ~exclude_mask)\n",
    "\n",
    "            # Inverse masks (excluding masked-out/self points)\n",
    "            nos_mask = ~os_mask & ~exclude_mask\n",
    "            nvs_mask = ~vs_mask & ~exclude_mask\n",
    "\n",
    "            # Collect all masks\n",
    "            mask_dict = {\n",
    "                \"AP\": ~exclude_mask,\n",
    "                \"OS\": os_mask,\n",
    "                \"VS\": vs_mask,\n",
    "                \"nOS\": nos_mask,\n",
    "                \"nVS\": nvs_mask\n",
    "            }\n",
    "\n",
    "            # Aggregate results\n",
    "            for key, mask in mask_dict.items():\n",
    "                valid_dists = dists[mask]\n",
    "                if valid_dists.numel() == 0:\n",
    "                    continue\n",
    "                Results[k][key][0] += valid_dists.numel()\n",
    "                Results[k][key][1] += valid_dists.sum()\n",
    "                Results[k][key][2] = torch.minimum(Results[k][key][2], valid_dists.min())\n",
    "\n",
    "    # Convert to CPU + readable format\n",
    "    final_results = [\n",
    "        {k: v.cpu().numpy().tolist() for k, v in layer_dict.items()}\n",
    "        for layer_dict in Results\n",
    "    ]\n",
    "\n",
    "    return final_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b1fc045-36c0-438a-a039-cda2937ffd00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/master/DAS/Complexity-vs.-Accuracy/notebooks/../das/Dataset_Generation.py:68: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
      "  return torch.tensor(model_inputs, dtype=torch.float32).to(device),torch.tensor(labels, dtype=torch.float32).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.39049801870714873 steps without improvement: 7 best accuracy: 0.9837\n",
      "Epoch 2, Loss: 0.026408711896237946 steps without improvement: 72 best accuracy: 0.9964\n",
      "Epoch 3, Loss: 0.008641325565577063 steps without improvement: 37 best accuracy: 0.9986\n",
      "Epoch 4, Loss: 0.004503298025667846 steps without improvement: 60 best accuracy: 0.9991\n",
      "Epoch 5, Loss: 0.0028202552880998155 steps without improvement: 138 best accuracy: 0.9997\n",
      "Epoch 6, Loss: 0.0019203961838059058 steps without improvement: 301 best accuracy: 0.9998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 164/164 [00:09<00:00, 16.72it/s]\n",
      "/tmp/ipykernel_65144/2711599703.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  feature_layers = torch.stack([torch.tensor(arr) for arr in Layer_Save[:-3]]).to(device)  # [num_layers, num_points, dim]\n",
      "/tmp/ipykernel_65144/2711599703.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  os_labels = torch.tensor(Layer_Save[-3]).to(device)\n",
      "/tmp/ipykernel_65144/2711599703.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  vs_labels = torch.stack([torch.tensor(Layer_Save[-2]), torch.tensor(Layer_Save[-1])], dim=1).to(device)\n",
      "Processing samples: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:22<00:00, 43.84it/s]\n",
      "1048576it [00:46, 22550.83it/s]\n",
      "1048576it [00:42, 24628.36it/s]\n",
      "1048576it [00:40, 25769.45it/s]\n",
      "1048576it [00:40, 26035.17it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 78\u001b[0m\n\u001b[1;32m     74\u001b[0m             seen\u001b[38;5;241m.\u001b[39madd(point)\n\u001b[1;32m     75\u001b[0m     Results[i][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOverlap\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(duplicates)\n\u001b[0;32m---> 78\u001b[0m \u001b[38;5;28mprint\u001b[39m(results)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "DEVICE  = sys.argv[1]#\"cuda\"/\"cpu\"\n",
    "num_classes=2\n",
    "\n",
    "ensure_directory_for_file(sys.argv[2])\n",
    "\n",
    "def register_intervention_hook(Save_array,Pos,layer):\n",
    "    def hook_fn(module, input, output):\n",
    "        Save_array[Pos].append(output.detach().cpu())\n",
    "    layer.register_forward_hook(hook_fn)\n",
    "\n",
    "\n",
    "Full_results=[]\n",
    "for acseed in [4287, 3837, 9097, 2635, 5137, 6442, 5234, 4641, 8039, 2266]:\n",
    "    set_seed(acseed)\n",
    "    X_train,y_train = make_model_dataset(1048576,4,DEVICE)#1048576\n",
    "    X_eval,y_eval   = make_model_dataset(10000,4,DEVICE)#10000\n",
    "    X_test,y_test   = make_model_dataset(10000,4,DEVICE)\n",
    "   \n",
    "    model,accuracy=make_model(X_train,y_train,X_eval,y_eval,X_test,y_test,input_size=16,epochs=20,device=DEVICE)\n",
    "    Layers=[]\n",
    "    Layers.append((\"Layer1\",model.mlp.h[0]))\n",
    "    Layers.append((\"Layer2\",model.mlp.h[1]))\n",
    "    Layers.append((\"Layer3\",model.mlp.h[2]))\n",
    "\n",
    "\n",
    "    Layer_Save=[[],[],[],[]]\n",
    "    Hooks=[]\n",
    "    predicted_classes_set = set()\n",
    "    for acpos,aclayer in enumerate(Layers):\n",
    "        Layer_Save.append([])\n",
    "        Hooks.append(register_intervention_hook(Layer_Save,acpos+1,aclayer[1]))\n",
    "    \n",
    "    \n",
    "    test_dataset = TensorDataset(X_train,y_train)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=6400, shuffle=False)#6400\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in tqdm(test_loader):\n",
    "            Layer_Save[0].append(X_batch)\n",
    "            output=model(X_batch)\n",
    "            predicted = torch.argmax(output, dim=1)\n",
    "            Layer_Save[-3].append(predicted)\n",
    "            Layer_Save[-2].append(torch.all(X_batch[:, :4] == X_batch[:, 4:8], dim=1))\n",
    "            Layer_Save[-1].append(torch.all(X_batch[:, 8:12] == X_batch[:, 12:16], dim=1))\n",
    "        \n",
    "            # Update set with predictions from this batch\n",
    "            predicted_classes_set.update(predicted.tolist())\n",
    "    \n",
    "    for i in range(len(Layer_Save)):\n",
    "        Layer_Save[i]=torch.cat(Layer_Save[i])\n",
    "\n",
    "    Results=[]\n",
    "    for _ in range(len(Layer_Save)-3):\n",
    "        Results.append({})\n",
    "        Results[-1][\"Overlap\"]=None\n",
    "        Results[-1][\"AP\"]=[0,0,np.inf]\n",
    "        Results[-1][\"OS\"]=[0,0,np.inf]\n",
    "        Results[-1][\"nOS\"]=[0,0,np.inf]\n",
    "        Results[-1][\"VS\"]=[0,0,np.inf]\n",
    "        Results[-1][\"nVS\"]=[0,0,np.inf]\n",
    "        \n",
    "    Results=analyze_layer_distances(Layer_Save, sample_size=10000, device='cuda')\n",
    "    \n",
    "    for i in range(len(Layer_Save)-3):\n",
    "        seen = set()\n",
    "        duplicates = []\n",
    "        \n",
    "        for point in tqdm(map(tuple, Layer_Save[i])):\n",
    "            if point in seen:\n",
    "                duplicates.append(point)\n",
    "            else:\n",
    "                seen.add(point)\n",
    "        Results[i][\"Overlap\"]=len(duplicates)\n",
    "\n",
    "    Full_results.append(Results)\n",
    "    \n",
    "    with open(sys.argv[2], 'w') as f:\n",
    "        json.dump(Full_results, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
