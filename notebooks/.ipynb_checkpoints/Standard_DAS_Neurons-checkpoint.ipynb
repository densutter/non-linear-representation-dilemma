{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f9c7a74-0fa0-4744-819a-12619cec2503",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import copy\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b227db55-31c7-4a24-8c73-b6aa17893a34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/dummy/dummy/runs/6shmn0tk?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x750ef9192750>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init(mode=\"disabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "288d0ed6-20a0-45d1-b3a3-e0f9dea87b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from das.Classification_Model import (MLPForClassification,\n",
    "                                  train_model,\n",
    "                                  eval_model,\n",
    "                                  test_model,\n",
    "                                  make_model)\n",
    "from das.Helper_Functions import set_seed\n",
    "from das.Dataset_Generation import (make_model_dataset,\n",
    "                                make_model_dataset_AndOrAnd,\n",
    "                                make_intervention_dataset_variable_intervention_all,\n",
    "                                make_intervention_dataset_variable_intervention_first,\n",
    "                                make_intervention_dataset_first_input_intervention,\n",
    "                                make_intervention_dataset_AndOrAnd,\n",
    "                                make_intervention_dataset_AndOr)\n",
    "from das.RevNet import RevNet\n",
    "from das.Rotation_Model import Rotation\n",
    "from das.DAS import phi_class\n",
    "from das.DAS_MLP import Distributed_Alignment_Search_MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69c97efd-716b-4155-a0c7-85ffa6295edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For different Settings and transformation functions please adapt this configurations:\n",
    "\n",
    "DEVICE  = \"cpu\" #\"cuda\"/\"cpu\"\n",
    "Setting = \"Both Equality Relations\"\n",
    "#Setting = \"Left Equality Relation\"\n",
    "#Setting = \"Identity of First Argument\"\n",
    "#Setting = \"AndOrAnd\"\n",
    "#Setting = \"AndOr\"\n",
    "\n",
    "\n",
    "transformation_config = {\"type\"        : \"identity\",\n",
    "                         \"in_features\" :         16} #24 for the Settings \"AndOrAnd\" and \"AndOr\"\n",
    "\n",
    "Max_Epochs                       = 50\n",
    "Early_Stopping_Epochs            = 5\n",
    "early_stopping_improve_threshold = 0.001\n",
    "ReduceLROnPlateau_patience       = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a3e4473-da77-4b93-83a4-751ec66ede90",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_variable_settings = [\"Identity of First Argument\",\"Left Equality Relation\"]\n",
    "two_variable_settings = [\"Both Equality Relations\",\"AndOrAnd\",\"AndOr\"]\n",
    "DAS_Original_tasks    = [\"Both Equality Relations\",\"Left Equality Relation\",\"Identity of First Argument\"]\n",
    "AndOrAnd_tasks        = [\"AndOrAnd\",\"AndOr\"]\n",
    "numvar=None\n",
    "if Setting in two_variable_settings:\n",
    "    numvar=2\n",
    "elif Setting in one_variable_settings:\n",
    "    numvar=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69f1b2b5-3f97-43cf-8b3b-90172d05e66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Process_Data_Left_Equality(Data):\n",
    "    for i in range(len(Data)):\n",
    "        Data[i][\"sources\"]=Data[i][\"sources\"][:1]\n",
    "        Data[i][\"intervention\"]=Data[i][\"intervention\"][:1]\n",
    "    return Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c5bc635-843b-481a-af1a-0657a3163e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_all_pairs(possible_list, used_list):\n",
    "    pairs = []\n",
    "    \n",
    "    for i in possible_list:\n",
    "        if i in used_list:\n",
    "            continue\n",
    "        for j in possible_list:\n",
    "            if j in used_list:\n",
    "                continue\n",
    "            if i==j:\n",
    "                continue\n",
    "            pairs.append([i,j])\n",
    "    \n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53e98d55-7e8a-4dec-872c-7b6b1ca6a144",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/master/DAS/Complexity-vs.-Accuracy/notebooks/../das/Dataset_Generation.py:64: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
      "  return torch.tensor(model_inputs, dtype=torch.float32).to(device),torch.tensor(labels, dtype=torch.float32).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.6992157101631165 steps without improvement: 0 best accuracy: 0.501\n",
      "Epoch 2, Loss: 0.6987351179122925 steps without improvement: 1 best accuracy: 0.501\n",
      "Epoch 3, Loss: 0.6982534527778625 steps without improvement: 2 best accuracy: 0.501\n",
      "Epoch 4, Loss: 0.6977711319923401 steps without improvement: 3 best accuracy: 0.501\n",
      "Epoch 5, Loss: 0.6973007321357727 steps without improvement: 4 best accuracy: 0.501\n",
      "Epoch 6, Loss: 0.6968293786048889 steps without improvement: 5 best accuracy: 0.501\n",
      "Epoch 7, Loss: 0.6963565349578857 steps without improvement: 6 best accuracy: 0.501\n",
      "Epoch 8, Loss: 0.6958898901939392 steps without improvement: 7 best accuracy: 0.501\n",
      "Epoch 9, Loss: 0.6954290270805359 steps without improvement: 8 best accuracy: 0.501\n",
      "Epoch 10, Loss: 0.6949644088745117 steps without improvement: 9 best accuracy: 0.501\n",
      "Epoch 11, Loss: 0.6944962739944458 steps without improvement: 10 best accuracy: 0.501\n",
      "Epoch 12, Loss: 0.6940276622772217 steps without improvement: 11 best accuracy: 0.501\n",
      "Epoch 13, Loss: 0.6935541033744812 steps without improvement: 12 best accuracy: 0.501\n",
      "Epoch 14, Loss: 0.6930786371231079 steps without improvement: 13 best accuracy: 0.501\n",
      "Epoch 15, Loss: 0.6926010847091675 steps without improvement: 14 best accuracy: 0.501\n",
      "Epoch 16, Loss: 0.6921173334121704 steps without improvement: 15 best accuracy: 0.501\n",
      "Epoch 17, Loss: 0.6916273832321167 steps without improvement: 16 best accuracy: 0.501\n",
      "Epoch 18, Loss: 0.6911311149597168 steps without improvement: 17 best accuracy: 0.501\n",
      "Epoch 19, Loss: 0.6906253099441528 steps without improvement: 18 best accuracy: 0.501\n",
      "Epoch 20, Loss: 0.6901108622550964 steps without improvement: 19 best accuracy: 0.501\n",
      "**  tensor(0.6983) 0.5082 [[0], [1]]\n",
      "**  tensor(0.6984) 0.5082 [[0], [2]]\n",
      "**  tensor(0.6984) 0.5082 [[0], [3]]\n",
      "**  tensor(0.6984) 0.5082 [[0], [4]]\n",
      "**  tensor(0.6984) 0.5082 [[0], [5]]\n",
      "**  tensor(0.6984) 0.5082 [[0], [6]]\n",
      "**  tensor(0.6984) 0.5082 [[0], [7]]\n",
      "**  tensor(0.6984) 0.5082 [[0], [8]]\n",
      "**  tensor(0.6984) 0.5082 [[0], [9]]\n",
      "**  tensor(0.6984) 0.5082 [[0], [10]]\n",
      "**  tensor(0.6984) 0.5082 [[0], [11]]\n",
      "**  tensor(0.6984) 0.5082 [[0], [12]]\n",
      "**  tensor(0.6984) 0.5082 [[0], [13]]\n",
      "**  tensor(0.6984) 0.5082 [[0], [14]]\n",
      "**  tensor(0.6984) 0.5082 [[0], [15]]\n",
      "**  tensor(0.6984) 0.5082 [[1], [0]]\n",
      "**  tensor(0.6984) 0.5082 [[1], [2]]\n",
      "**  tensor(0.6984) 0.5082 [[1], [3]]\n",
      "**  tensor(0.6984) 0.5082 [[1], [4]]\n",
      "**  tensor(0.6984) 0.5082 [[1], [5]]\n",
      "**  tensor(0.6984) 0.5082 [[1], [6]]\n",
      "**  tensor(0.6984) 0.5082 [[1], [7]]\n",
      "**  tensor(0.6984) 0.5082 [[1], [8]]\n",
      "**  tensor(0.6984) 0.5082 [[1], [9]]\n",
      "**  tensor(0.6984) 0.5082 [[1], [10]]\n",
      "**  tensor(0.6984) 0.5082 [[1], [11]]\n",
      "**  tensor(0.6984) 0.5082 [[1], [12]]\n",
      "**  tensor(0.6984) 0.5082 [[1], [13]]\n",
      "**  tensor(0.6984) 0.5082 [[1], [14]]\n",
      "**  tensor(0.6984) 0.5082 [[1], [15]]\n",
      "**  tensor(0.6984) 0.5082 [[2], [0]]\n",
      "**  tensor(0.6984) 0.5082 [[2], [1]]\n",
      "**  tensor(0.6984) 0.5082 [[2], [3]]\n",
      "**  tensor(0.6984) 0.5082 [[2], [4]]\n",
      "**  tensor(0.6984) 0.5082 [[2], [5]]\n",
      "**  tensor(0.6984) 0.5082 [[2], [6]]\n",
      "**  tensor(0.6984) 0.5082 [[2], [7]]\n",
      "**  tensor(0.6984) 0.5082 [[2], [8]]\n",
      "**  tensor(0.6984) 0.5082 [[2], [9]]\n",
      "**  tensor(0.6984) 0.5082 [[2], [10]]\n",
      "**  tensor(0.6984) 0.5082 [[2], [11]]\n",
      "**  tensor(0.6984) 0.5082 [[2], [12]]\n",
      "**  tensor(0.6984) 0.5082 [[2], [13]]\n",
      "**  tensor(0.6984) 0.5082 [[2], [14]]\n",
      "**  tensor(0.6984) 0.5082 [[2], [15]]\n",
      "**  tensor(0.6984) 0.5082 [[3], [0]]\n",
      "**  tensor(0.6984) 0.5082 [[3], [1]]\n",
      "**  tensor(0.6984) 0.5082 [[3], [2]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 88\u001b[0m\n\u001b[1;32m     84\u001b[0m _,Dloss\u001b[38;5;241m=\u001b[39mDAS_Experiment\u001b[38;5;241m.\u001b[39mtrain_test(batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6400\u001b[39m,\n\u001b[1;32m     85\u001b[0m                                    mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     86\u001b[0m                                    report_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;66;03m#Test\u001b[39;00m\n\u001b[1;32m     87\u001b[0m DAS_Experiment\u001b[38;5;241m.\u001b[39mTest_Dataset,DAS_Experiment\u001b[38;5;241m.\u001b[39mTest_Samples_Number\u001b[38;5;241m=\u001b[39mDAS_Experiment\u001b[38;5;241m.\u001b[39mPrepare_Dataset(DAS_Test)\n\u001b[0;32m---> 88\u001b[0m accur\u001b[38;5;241m=\u001b[39mDAS_Experiment\u001b[38;5;241m.\u001b[39mtrain_test(batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6400\u001b[39m,\n\u001b[1;32m     89\u001b[0m                                     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,)\u001b[38;5;66;03m#Test\u001b[39;00m\n\u001b[1;32m     90\u001b[0m DAS_Experiment\u001b[38;5;241m.\u001b[39mCleanup()\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m** \u001b[39m\u001b[38;5;124m\"\u001b[39m,Dloss,accuracy,DAS_Experiment\u001b[38;5;241m.\u001b[39mVariable_Dimensions)\n",
      "File \u001b[0;32m~/DAS/Complexity-vs.-Accuracy/notebooks/../das/DAS.py:270\u001b[0m, in \u001b[0;36mDistributed_Alignment_Search.train_test\u001b[0;34m(self, batch_size, epochs, mode, early_stopping_threshold, early_stopping_improve_threshold, TrainModel, verbose, val_every_n_steps, lr_warmup_steps, report_loss)\u001b[0m\n\u001b[1;32m    267\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m \u001b[38;5;66;03m# Exit batch loop\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m: \u001b[38;5;66;03m# Testing Step\u001b[39;00m\n\u001b[0;32m--> 270\u001b[0m      loss, batch_correct, batch_samples, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_Batch(mode, data, ac_batch, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    271\u001b[0m      total_correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_correct\n\u001b[1;32m    272\u001b[0m      total_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_samples\n",
      "File \u001b[0;32m~/DAS/Complexity-vs.-Accuracy/notebooks/../das/DAS_MLP.py:90\u001b[0m, in \u001b[0;36mDistributed_Alignment_Search_MLP.process_Batch\u001b[0;34m(self, mode, data, ac_batch, total_correct, total_samples)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_activations \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(ac_batch), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mHidden_Layer_Size)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mDevice)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ac_source_pos \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msources\u001b[39m\u001b[38;5;124m\"\u001b[39m])):\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;66;03m#add info needed for optimization... no sense in running inputs who are not used:\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m     used_source_indices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract_sources_to_run(ac_source_pos,ac_batch,data)\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(used_source_indices)\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     92\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode_info\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m,ac_source_pos,used_source_indices]\n",
      "File \u001b[0;32m~/DAS/Complexity-vs.-Accuracy/notebooks/../das/DAS_MLP.py:125\u001b[0m, in \u001b[0;36mDistributed_Alignment_Search_MLP.extract_sources_to_run\u001b[0;34m(self, which_variable, batch_indices, data)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ip, i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch_indices):\n\u001b[1;32m    124\u001b[0m     is_used\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mintervention\u001b[39m\u001b[38;5;124m\"\u001b[39m][i][\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mVariable_Dimensions[which_variable][\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_used:\n\u001b[1;32m    126\u001b[0m         used_source_indices\u001b[38;5;241m.\u001b[39mappend(ip)\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m used_source_indices\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "results=[]\n",
    "for acseed in [4287, 3837, 9097, 2635, 5137, 6442, 5234, 4641, 8039, 2266]:\n",
    "    results.append({})\n",
    "    set_seed(acseed)\n",
    "    if Setting in DAS_Original_tasks:\n",
    "        X_train,y_train = make_model_dataset(1048576,4,DEVICE)\n",
    "        X_eval,y_eval   = make_model_dataset(10000,4,DEVICE)\n",
    "        X_test,y_test   = make_model_dataset(10000,4,DEVICE)\n",
    "    elif Setting in AndOrAnd_tasks:\n",
    "        X_train,y_train = make_model_dataset_AndOrAnd(1048576,4,DEVICE)\n",
    "        X_eval,y_eval   = make_model_dataset_AndOrAnd(10000,4,DEVICE)\n",
    "        X_test,y_test   = make_model_dataset_AndOrAnd(10000,4,DEVICE)\n",
    "    #print(\"!!!!!!!!!!!!! Set and training number and epochs back to 20\")\n",
    "    model,accuracy=make_model(X_train,y_train,X_eval,y_eval,X_test,y_test,input_size=transformation_config[\"in_features\"],epochs=20,device=DEVICE)\n",
    "    Layers=[]\n",
    "    Layers.append((\"Layer1\",model.mlp.h[0]))\n",
    "    Layers.append((\"Layer2\",model.mlp.h[1]))\n",
    "    Layers.append((\"Layer3\",model.mlp.h[2]))\n",
    "    inter_dims=[]\n",
    "    \n",
    "\n",
    "    if Setting == \"Both Equality Relations\":\n",
    "        DAS_Train = make_intervention_dataset_variable_intervention_all(1,4)\n",
    "        DAS_Test  = make_intervention_dataset_variable_intervention_all(6400,4)\n",
    "        DAS_Eval  = make_intervention_dataset_variable_intervention_all(6400,4)\n",
    "    elif Setting == \"Left Equality Relation\":\n",
    "        DAS_Train = Process_Data_Left_Equality(make_intervention_dataset_variable_intervention_first(1,4))\n",
    "        DAS_Test  = Process_Data_Left_Equality(make_intervention_dataset_variable_intervention_first(6400,4))\n",
    "        DAS_Eval  = Process_Data_Left_Equality(make_intervention_dataset_variable_intervention_first(6400,4))\n",
    "    elif Setting == \"Identity of First Argument\":\n",
    "        DAS_Train = make_intervention_dataset_first_input_intervention(1,4)\n",
    "        DAS_Test  = make_intervention_dataset_first_input_intervention(6400,4)\n",
    "        DAS_Eval  = make_intervention_dataset_first_input_intervention(6400,4)\n",
    "    elif Setting == \"AndOrAnd\":\n",
    "        DAS_Train = make_intervention_dataset_AndOrAnd(1,4)\n",
    "        DAS_Test  = make_intervention_dataset_AndOrAnd(6400,4)\n",
    "        DAS_Eval  = make_intervention_dataset_AndOrAnd(6400,4)\n",
    "    elif Setting == \"AndOr\":\n",
    "        DAS_Train = make_intervention_dataset_AndOr(1,4)\n",
    "        DAS_Test  = make_intervention_dataset_AndOr(6400,4)\n",
    "        DAS_Eval  = make_intervention_dataset_AndOr(6400,4)\n",
    "    else:\n",
    "        Exception(\"Unknown Setting\")\n",
    "        \n",
    "    results[-1][\"accuracy\"]=accuracy\n",
    "    for LayerName,Layer in Layers:\n",
    "        results[-1][LayerName]={}\n",
    "            \n",
    "    \n",
    "        p = torch.nn.Identity()\n",
    "        p.to(DEVICE)\n",
    "        p_inverse = torch.nn.Identity()\n",
    "        optimizer = None #optim.Adam(p.parameters(), lr=0.001)\n",
    "        scheduler = None #optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=ReduceLROnPlateau_patience)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        \n",
    "        phi=phi_class(p,p_inverse,criterion,optimizer,scheduler)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if Setting in two_variable_settings:\n",
    "            neuronset=[[],[]]\n",
    "            results[-1][LayerName]=[]\n",
    "            while len(neuronset[0])<transformation_config[\"in_features\"]//2:\n",
    "                pairs=make_all_pairs(list(range(transformation_config[\"in_features\"])),neuronset[0]+neuronset[1])\n",
    "                best_pair_neurons=None\n",
    "                best_pair_acc=-1\n",
    "                best_pair_loss=None\n",
    "                for pair in pairs:\n",
    "                    ac_neuronset=copy.deepcopy(neuronset)\n",
    "                    ac_neuronset[0].append(pair[0])\n",
    "                    ac_neuronset[1].append(pair[1])\n",
    "                    DAS_Experiment=Distributed_Alignment_Search_MLP(Model=model,\n",
    "                                                                    Model_Layer=Layer,\n",
    "                                                                    Train_Data_Raw=DAS_Train,\n",
    "                                                                    Test_Data_Raw=DAS_Eval,\n",
    "                                                                    Eval_Data_Raw=DAS_Train,\n",
    "                                                                    Hidden_Layer_Size=transformation_config[\"in_features\"],\n",
    "                                                                    Variable_Dimensions=ac_neuronset,\n",
    "                                                                    Transformation_Class=phi,\n",
    "                                                                    Device=DEVICE)\n",
    "                    _,Dloss=DAS_Experiment.train_test(batch_size=6400,\n",
    "                                                       mode=2,\n",
    "                                                       report_loss=True)#Test\n",
    "                    DAS_Experiment.Test_Dataset,DAS_Experiment.Test_Samples_Number=DAS_Experiment.Prepare_Dataset(DAS_Test)\n",
    "                    accur=DAS_Experiment.train_test(batch_size=6400,\n",
    "                                                        mode=2,)#Test\n",
    "                    DAS_Experiment.Cleanup()\n",
    "                    if (best_pair_loss is None) or best_pair_loss>Dloss:\n",
    "                        best_pair_loss=Dloss\n",
    "                        best_pair_acc=accur\n",
    "                        best_pair_neurons=ac_neuronset\n",
    "                results[-1][LayerName].append((best_pair_neurons,best_pair_acc))\n",
    "                neuronset=copy.deepcopy(best_pair_neurons)\n",
    "        if Setting in one_variable_settings:\n",
    "            neuronset=[[]]\n",
    "            results[-1][LayerName]=[]\n",
    "            while len(neuronset[0])<transformation_config[\"in_features\"]//2:\n",
    "                best_pair_neurons=None\n",
    "                best_pair_acc=-1\n",
    "                for element in list(range(transformation_config[\"in_features\"])):\n",
    "                    if element in neuronset[0]:\n",
    "                        continue\n",
    "                    ac_neuronset=copy.deepcopy(neuronset)\n",
    "                    ac_neuronset[0].append(element)\n",
    "                    DAS_Experiment=Distributed_Alignment_Search_MLP(Model=model,\n",
    "                                                                    Model_Layer=Layer,\n",
    "                                                                    Train_Data_Raw=DAS_Train,\n",
    "                                                                    Test_Data_Raw=DAS_Eval,\n",
    "                                                                    Eval_Data_Raw=DAS_Train,\n",
    "                                                                    Hidden_Layer_Size=transformation_config[\"in_features\"],\n",
    "                                                                    Variable_Dimensions=ac_neuronset,\n",
    "                                                                    Transformation_Class=phi,\n",
    "                                                                    Device=DEVICE)\n",
    "                    _,Dloss=DAS_Experiment.train_test(batch_size=6400,\n",
    "                                                       mode=2,\n",
    "                                                       report_loss=True)\n",
    "                    DAS_Experiment.Test_Dataset,DAS_Experiment.Test_Samples_Number=DAS_Experiment.Prepare_Dataset(DAS_Test)\n",
    "                    accur=DAS_Experiment.train_test(batch_size=6400,\n",
    "                                                        mode=2,)#Test\n",
    "                    DAS_Experiment.Cleanup()\n",
    "                    if (best_pair_loss is None) or best_pair_loss>Dloss:\n",
    "                        best_pair_loss=Dloss\n",
    "                        best_pair_acc=accur\n",
    "                        best_pair_neurons=ac_neuronset\n",
    "                results[-1][LayerName].append((best_pair_neurons,best_pair_acc))\n",
    "                neuronset=copy.deepcopy(best_pair_neurons)\n",
    "\n",
    "        \n",
    "        DAS_Experiment=None\n",
    "        with open('results.json', 'w') as f:\n",
    "            json.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c3e214-d05a-4c5e-bada-317d4e2b9f99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
