{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96f982de-5e1b-480c-a4ae-fc80b31a91a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import copy\n",
    "import json\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "921f3fcc-56a9-44b3-a2e0-8ff95f9879f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Helper_Functions import set_seed\n",
    "from Dataset_Generation import Generate_LLM_Eval_Intervention_Data\n",
    "from LLM_Model import (make_model,\n",
    "                       LLM_Criterion)\n",
    "from RevNet import RevNet\n",
    "from Rotation_Model import Rotation\n",
    "from DAS import phi_class\n",
    "from DAS_LLM import Distributed_Alignment_Search_LLM\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f9c7a74-0fa0-4744-819a-12619cec2503",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_config = {\"model\"   : \"meta-llama/Llama-3.2-1B\",\n",
    "                \"Trained\" :                         2} \n",
    "#Trained=0:pretrained, 1:fully randomized, 2:only randomize llm head, 3: only randomize embedding, 4: randomize linked embedding and lm head\n",
    "\n",
    "DEVICE        = \"cuda:0\" #\"cuda:0\" #\"cuda\"/\"cpu\"\n",
    "\n",
    "\"\"\"\n",
    "transformation_config = {\"type\"        : \"Rotation\",\n",
    "                         \"in_features\" :       2048}\n",
    "\"\"\"\n",
    "transformation_config = {\"type\"          : \"RevNet\",\n",
    "                         \"number_blocks\" :       10,\n",
    "                         \"in_features\"   :     2048,\n",
    "                         \"hidden_size\"   :       16}\n",
    "\n",
    "Max_Epochs                       = 100 #4 #1 #50\n",
    "Early_Stopping_Epochs            = 100 #4 #1 #50\n",
    "early_stopping_improve_threshold = 0.001\n",
    "LLM_test_samples                 = 1600\n",
    "Intervention_train_size          = 1280000//8\n",
    "Intervention_eval_size           = 1600\n",
    "Intervention_test_size           = 1600\n",
    "learning_rate                    = 0.0001#0.000001\n",
    "ReduceLROnPlateau_patience       = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "948b1b7d-4ce3-45d1-b1e4-fe4638a782df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████| 200000/200000 [00:00<00:00, 328566.62it/s]\n",
      "100%|█████████████████████████████████████| 1600/1600 [00:00<00:00, 2185.53it/s]\n",
      "100%|█████████████████████████████████| 160000/160000 [01:03<00:00, 2514.76it/s]\n",
      "100%|█████████████████████████████████████| 1600/1600 [00:00<00:00, 2839.37it/s]\n",
      "100%|█████████████████████████████████████| 1600/1600 [00:00<00:00, 2833.16it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_config[\"model\"])\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "LLM_test_data,DAS_Train,DAS_Eval,DAS_Test=Generate_LLM_Eval_Intervention_Data(filename='./mecha_ioi_200k.parquet',\n",
    "                                                                              tokenizer=tokenizer,\n",
    "                                                                              LLM_test_samples=LLM_test_samples,\n",
    "                                                                              Intervention_train_size=Intervention_train_size,\n",
    "                                                                              Intervention_eval_size=Intervention_eval_size,\n",
    "                                                                              Intervention_test_size=Intervention_test_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4aaa7a3-a684-4fb1-9381-941b82d67223",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1600/1600 [00:32<00:00, 48.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.4556\n",
      "Layer9 : 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                 | 0/10000 [00:00<?, ?it/s]You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|███████████████████████████████████| 10000/10000 [1:12:43<00:00,  2.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 100/100 [00:35<00:00,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.7313303010463714 steps without improvement: 0 eval accuracy: 0.49625 best eval accuracy: 0.49625 learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|████▎                               | 1198/10000 [09:06<1:06:06,  2.22it/s]"
     ]
    }
   ],
   "source": [
    "results=[]\n",
    "for acseed in [4287]:\n",
    "    results.append({})\n",
    "    set_seed(acseed)\n",
    "    model,accuracy=make_model(model_config[\"model\"],LLM_test_data,model_config[\"Trained\"],device=DEVICE)\n",
    "    Layers=[]\n",
    "    #Layers.append((\"Layer7\",model.model.layers[7]))\n",
    "    Layers.append((\"Layer9\",model.model.layers[9]))\n",
    "    #Layers.append((\"Layer15\",model.model.layers[15]))\n",
    "    inter_dims=[]\n",
    "    inter_dims.append([list(range(0,transformation_config[\"in_features\"]//2))])\n",
    "    #inter_dims.append([list(range(0,transformation_config[\"in_features\"]//64))])\n",
    "    #inter_dims.append([list(range(0,1))])\n",
    "    \n",
    "    results[-1][\"accuracy\"]=accuracy\n",
    "    for LayerName,Layer in Layers:\n",
    "        results[-1][LayerName]={}\n",
    "        for inter_dim in inter_dims:\n",
    "            print(LayerName,\":\",len(inter_dim[0]), flush=True)\n",
    "            #Initialize transformation function\n",
    "            \n",
    "            #Initialize transformation function\n",
    "            if transformation_config[\"type\"]==\"Rotation\":\n",
    "                p = Rotation(transformation_config[\"in_features\"])\n",
    "            elif transformation_config[\"type\"]==\"RevNet\":\n",
    "                p = RevNet(number_blocks =  transformation_config[\"number_blocks\"],\n",
    "                           in_features   =  transformation_config[\"in_features\"],\n",
    "                           hidden_size   =  transformation_config[\"hidden_size\"]\n",
    "                          )\n",
    "            else:\n",
    "                Exception(\"Unknown transformation function\")\n",
    "            p.to(DEVICE)\n",
    "            p_inverse = p.inverse\n",
    "            optimizer = optim.Adam(p.parameters(), lr=learning_rate)\n",
    "            scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=ReduceLROnPlateau_patience)\n",
    "            criterion = LLM_Criterion\n",
    "            \n",
    "            \n",
    "            phi=phi_class(p,p_inverse,LLM_Criterion,optimizer,scheduler)\n",
    "\n",
    "            \n",
    "    \n",
    "            DAS_Experiment=Distributed_Alignment_Search_LLM(Model=model,\n",
    "                                                            Model_Layer=Layer,\n",
    "                                                            Train_Data_Raw=DAS_Train,\n",
    "                                                            Test_Data_Raw=DAS_Test,\n",
    "                                                            Eval_Data_Raw=DAS_Eval,\n",
    "                                                            Hidden_Layer_Size=transformation_config[\"in_features\"],\n",
    "                                                            Variable_Dimensions=inter_dim,\n",
    "                                                            Transformation_Class=phi,\n",
    "                                                            Device=DEVICE,\n",
    "                                                            tokenizer=tokenizer)\n",
    "    \n",
    "            DAS_Experiment.train_test(batch_size=16,\n",
    "                                      epochs=Max_Epochs,\n",
    "                                      mode=1,\n",
    "                                      early_stopping_threshold=Early_Stopping_Epochs,\n",
    "                                      early_stopping_improve_threshold=early_stopping_improve_threshold,\n",
    "                                      verbose=True) #Train\n",
    "    \n",
    "            accuracy=DAS_Experiment.train_test(batch_size=32,\n",
    "                                               mode=2,\n",
    "                                               verbose=True)#Test\n",
    "            results[-1][LayerName][str(inter_dim)]=accuracy\n",
    "            DAS_Experiment.Cleanup()\n",
    "            DAS_Experiment=None\n",
    "            with open('results.json', 'w') as f:\n",
    "                json.dump(results, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
