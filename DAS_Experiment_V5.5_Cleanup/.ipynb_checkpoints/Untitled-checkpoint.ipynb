{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12346d68-c5bc-481e-9217-857ca58198ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e179523-6ba4-42e9-9da9-045dcf6d62a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "A=\"\"\"    def train_test(self,batch_size,epochs=1,mode=1,early_stopping_threshold=3): #1=train,2=test,3=eval\n",
    "\n",
    "        self.Model.eval()\n",
    "        for param in self.Model.parameters():\n",
    "            param.requires_grad = False  # This freezes the weights\n",
    "\n",
    "        if mode==1:\n",
    "            self.Transformation_Class.phi.train()\n",
    "            for param in self.Transformation_Class.phi.parameters():\n",
    "                param.requires_grad = True  # This unfreezes the weights\n",
    "            data=self.Train_Dataset\n",
    "            Sample_Indices=list(range(self.Train_Sample_Number))\n",
    "        elif mode==2:\n",
    "            self.Transformation_Class.phi.eval()\n",
    "            for param in self.Transformation_Class.phi.parameters():\n",
    "                param.requires_grad = False  # This freezes the weights\n",
    "            data=self.Test_Dataset\n",
    "            Sample_Indices=list(range(self.Test_Samples_Number))\n",
    "        elif mode==3:\n",
    "            self.Transformation_Class.phi.eval()\n",
    "            for param in self.Transformation_Class.phi.parameters():\n",
    "                param.requires_grad = False  # This freezes the weights\n",
    "            data=self.Eval_Dataset\n",
    "            Sample_Indices=list(range(self.Eval_Samples_Number))\n",
    "        \n",
    "        best_accuracy=-1\n",
    "        steps_without_improvement=0\n",
    "        Best_Phi=None\n",
    "        for epoch in range(epochs):\n",
    "            total_correct = 0\n",
    "            total_samples = 0\n",
    "            total_loss = 0\n",
    "        \n",
    "            #Make Batches\n",
    "            random.shuffle(Sample_Indices)\n",
    "            DAS_Train_Batches=self.chunk_list(Sample_Indices, batch_size)\n",
    "\n",
    "            \n",
    "            for ac_batch in tqdm(DAS_Train_Batches):\n",
    "\n",
    "                if mode==1: \n",
    "                    self.Transformation_Class.optimizer.zero_grad()\n",
    "                    \n",
    "                loss,total_correct,total_samples = self.process_Batch(mode,data,ac_batch,total_correct,total_samples)\n",
    "                \n",
    "                if mode==1:\n",
    "                    loss.backward()\n",
    "                    self.Transformation_Class.optimizer.step()\n",
    "                    total_loss += loss.item()\n",
    "            \n",
    "            if mode==1:\n",
    "                eval_acc=self.train_test(batch_size,epochs=1,mode=3)\n",
    "                if eval_acc>best_accuracy:\n",
    "                    best_accuracy=eval_acc\n",
    "                    steps_without_improvement=0\n",
    "                    Best_Phi=copy.deepcopy(self.Transformation_Class.phi)\n",
    "                else:\n",
    "                    steps_without_improvement+=1\n",
    "                if steps_without_improvement>=early_stopping_threshold or epoch==epochs-1:\n",
    "                    Best_Phi.to(self.Device)\n",
    "                    Best_Phi_inverse=InverseTransformation_Function(Best_Phi)\n",
    "                    optimizer = optim.Adam(Best_Phi.parameters(), lr=0.001)\n",
    "                    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=10)\n",
    "                    self.Transformation_Class=phi_class(Best_Phi,Best_Phi_inverse,LLM_Criterion,optimizer,scheduler)\n",
    "                    break\n",
    "                total_loss=total_loss / len(DAS_Train_Batches)\n",
    "                self.Transformation_Class.scheduler.step(total_loss)\n",
    "                print(f\"Epoch {epoch+1}, Loss: {total_loss}\",\n",
    "                      \"steps without improvement:\",steps_without_improvement,\n",
    "                      \"best accuracy:\",best_accuracy,\n",
    "                      \"learning rate:\",self.Transformation_Class.scheduler.get_last_lr())\n",
    "                \n",
    "        if mode==2 or mode==3: \n",
    "            self.Transformation_Class.phi.train()\n",
    "            for param in self.Transformation_Class.phi.parameters():\n",
    "                param.requires_grad = True  # This unfreezes the weights\n",
    "            accuracy = total_correct / total_samples\n",
    "            return accuracy\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "961ae313-96c1-4bcf-a0a9-9b674fcca460",
   "metadata": {},
   "outputs": [],
   "source": [
    "B=\"\"\"    def train_test(self,batch_size,epochs=1,mode=1,early_stopping_threshold=3): #1=train,2=test,3=eval\n",
    "\n",
    "        self.Model.eval()\n",
    "        for param in self.Model.parameters():\n",
    "            param.requires_grad = False  # This freezes the weights\n",
    "\n",
    "        if mode==1:\n",
    "            self.Transformation_Class.phi.train()\n",
    "            for param in self.Transformation_Class.phi.parameters():\n",
    "                param.requires_grad = True  # This unfreezes the weights\n",
    "            data=self.Train_Dataset\n",
    "            Sample_Indices=list(range(self.Train_Sample_Number))\n",
    "        elif mode==2:\n",
    "            self.Transformation_Class.phi.eval()\n",
    "            for param in self.Transformation_Class.phi.parameters():\n",
    "                param.requires_grad = False  # This freezes the weights\n",
    "            data=self.Test_Dataset\n",
    "            Sample_Indices=list(range(self.Test_Samples_Number))\n",
    "        elif mode==3:\n",
    "            self.Transformation_Class.phi.eval()\n",
    "            for param in self.Transformation_Class.phi.parameters():\n",
    "                param.requires_grad = False  # This freezes the weights\n",
    "            data=self.Eval_Dataset\n",
    "            Sample_Indices=list(range(self.Eval_Samples_Number))\n",
    "        \n",
    "        best_accuracy=-1\n",
    "        steps_without_improvement=0\n",
    "        Best_Phi=None\n",
    "        for epoch in range(epochs):\n",
    "            total_correct = 0\n",
    "            total_samples = 0\n",
    "            total_loss = 0\n",
    "        \n",
    "            #Make Batches\n",
    "            random.shuffle(Sample_Indices)\n",
    "            DAS_Train_Batches=self.chunk_list(Sample_Indices, batch_size)\n",
    "            \n",
    "            for ac_batch in DAS_Train_Batches:\n",
    "\n",
    "                if mode==1: \n",
    "                    self.Transformation_Class.optimizer.zero_grad()\n",
    "                \n",
    "                \n",
    "                loss,total_correct,total_samples = self.process_Batch(self,mode,data,ac_batch,total_correct,total_samples)\n",
    "                \n",
    "                if mode==1:\n",
    "                    loss.backward()\n",
    "                    self.Transformation_Class.optimizer.step()\n",
    "                    total_loss += loss.item()\n",
    "\n",
    "            if mode==1:\n",
    "                eval_acc=self.train_test(batch_size,epochs=1,mode=3)\n",
    "                if eval_acc>best_accuracy:\n",
    "                    best_accuracy=eval_acc\n",
    "                    steps_without_improvement=0\n",
    "                    Best_Phi=copy.deepcopy(self.Transformation_Class.phi)\n",
    "                else:\n",
    "                    steps_without_improvement+=1\n",
    "                if steps_without_improvement>=early_stopping_threshold or epoch==epochs-1:\n",
    "                    Best_Phi.to(self.Device)\n",
    "                    Best_Phi_inverse=Best_Phi.inverse\n",
    "                    criterion = nn.CrossEntropyLoss()\n",
    "                    optimizer = optim.Adam(Best_Phi.parameters(), lr=0.001)\n",
    "                    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=10)\n",
    "                    self.Transformation_Class=phi_class(Best_Phi,Best_Phi_inverse,criterion,optimizer,scheduler)\n",
    "                    break\n",
    "                total_loss=total_loss / len(DAS_Train_Batches)\n",
    "                self.Transformation_Class.scheduler.step(total_loss)\n",
    "                print(f\"Epoch {epoch+1}, Loss: {total_loss}\",\n",
    "                      \"steps without improvement:\",steps_without_improvement,\n",
    "                      \"best accuracy:\",best_accuracy,\n",
    "                      \"learning rate:\",self.Transformation_Class.scheduler.get_last_lr())\n",
    "                \n",
    "        if mode==2 or mode==3: \n",
    "            self.Transformation_Class.phi.train()\n",
    "            for param in self.Transformation_Class.phi.parameters():\n",
    "                param.requires_grad = True  # This unfreezes the weights\n",
    "            accuracy = total_correct / total_samples\n",
    "            return accuracy\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c67e0f4-6983-431a-82d9-2108994a31ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['      def train_test(self,batch_size,epochs=1,mode=1,early_stopping_threshold=3): #1=train,2=test,3=eval\\n',\n",
       " '  \\n',\n",
       " '          self.Model.eval()\\n',\n",
       " '          for param in self.Model.parameters():\\n',\n",
       " '              param.requires_grad = False  # This freezes the weights\\n',\n",
       " '  \\n',\n",
       " '          if mode==1:\\n',\n",
       " '              self.Transformation_Class.phi.train()\\n',\n",
       " '              for param in self.Transformation_Class.phi.parameters():\\n',\n",
       " '                  param.requires_grad = True  # This unfreezes the weights\\n',\n",
       " '              data=self.Train_Dataset\\n',\n",
       " '              Sample_Indices=list(range(self.Train_Sample_Number))\\n',\n",
       " '          elif mode==2:\\n',\n",
       " '              self.Transformation_Class.phi.eval()\\n',\n",
       " '              for param in self.Transformation_Class.phi.parameters():\\n',\n",
       " '                  param.requires_grad = False  # This freezes the weights\\n',\n",
       " '              data=self.Test_Dataset\\n',\n",
       " '              Sample_Indices=list(range(self.Test_Samples_Number))\\n',\n",
       " '          elif mode==3:\\n',\n",
       " '              self.Transformation_Class.phi.eval()\\n',\n",
       " '              for param in self.Transformation_Class.phi.parameters():\\n',\n",
       " '                  param.requires_grad = False  # This freezes the weights\\n',\n",
       " '              data=self.Eval_Dataset\\n',\n",
       " '              Sample_Indices=list(range(self.Eval_Samples_Number))\\n',\n",
       " '          \\n',\n",
       " '          best_accuracy=-1\\n',\n",
       " '          steps_without_improvement=0\\n',\n",
       " '          Best_Phi=None\\n',\n",
       " '          for epoch in range(epochs):\\n',\n",
       " '              total_correct = 0\\n',\n",
       " '              total_samples = 0\\n',\n",
       " '              total_loss = 0\\n',\n",
       " '          \\n',\n",
       " '              #Make Batches\\n',\n",
       " '              random.shuffle(Sample_Indices)\\n',\n",
       " '              DAS_Train_Batches=self.chunk_list(Sample_Indices, batch_size)\\n',\n",
       " '- \\n',\n",
       " '              \\n',\n",
       " '-             for ac_batch in tqdm(DAS_Train_Batches):\\n',\n",
       " '?                             -----                 -\\n',\n",
       " '+             for ac_batch in DAS_Train_Batches:\\n',\n",
       " '  \\n',\n",
       " '                  if mode==1: \\n',\n",
       " '                      self.Transformation_Class.optimizer.zero_grad()\\n',\n",
       " '-                     \\n',\n",
       " '? ----\\n',\n",
       " '+                 \\n',\n",
       " '+                 \\n',\n",
       " '-                 loss,total_correct,total_samples = self.process_Batch(mode,data,ac_batch,total_correct,total_samples)\\n',\n",
       " '+                 loss,total_correct,total_samples = self.process_Batch(self,mode,data,ac_batch,total_correct,total_samples)\\n',\n",
       " '?                                                                       +++++\\n',\n",
       " '                  \\n',\n",
       " '                  if mode==1:\\n',\n",
       " '                      loss.backward()\\n',\n",
       " '                      self.Transformation_Class.optimizer.step()\\n',\n",
       " '                      total_loss += loss.item()\\n',\n",
       " '-             \\n',\n",
       " '+ \\n',\n",
       " '              if mode==1:\\n',\n",
       " '                  eval_acc=self.train_test(batch_size,epochs=1,mode=3)\\n',\n",
       " '                  if eval_acc>best_accuracy:\\n',\n",
       " '                      best_accuracy=eval_acc\\n',\n",
       " '                      steps_without_improvement=0\\n',\n",
       " '                      Best_Phi=copy.deepcopy(self.Transformation_Class.phi)\\n',\n",
       " '                  else:\\n',\n",
       " '                      steps_without_improvement+=1\\n',\n",
       " '                  if steps_without_improvement>=early_stopping_threshold or epoch==epochs-1:\\n',\n",
       " '                      Best_Phi.to(self.Device)\\n',\n",
       " '-                     Best_Phi_inverse=InverseTransformation_Function(Best_Phi)\\n',\n",
       " '+                     Best_Phi_inverse=Best_Phi.inverse\\n',\n",
       " '+                     criterion = nn.CrossEntropyLoss()\\n',\n",
       " '                      optimizer = optim.Adam(Best_Phi.parameters(), lr=0.001)\\n',\n",
       " '                      scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=10)\\n',\n",
       " '-                     self.Transformation_Class=phi_class(Best_Phi,Best_Phi_inverse,LLM_Criterion,optimizer,scheduler)\\n',\n",
       " '?                                                                                   ^^^^^\\n',\n",
       " '+                     self.Transformation_Class=phi_class(Best_Phi,Best_Phi_inverse,criterion,optimizer,scheduler)\\n',\n",
       " '?                                                                                   ^\\n',\n",
       " '                      break\\n',\n",
       " '                  total_loss=total_loss / len(DAS_Train_Batches)\\n',\n",
       " '                  self.Transformation_Class.scheduler.step(total_loss)\\n',\n",
       " '                  print(f\"Epoch {epoch+1}, Loss: {total_loss}\",\\n',\n",
       " '                        \"steps without improvement:\",steps_without_improvement,\\n',\n",
       " '                        \"best accuracy:\",best_accuracy,\\n',\n",
       " '                        \"learning rate:\",self.Transformation_Class.scheduler.get_last_lr())\\n',\n",
       " '                  \\n',\n",
       " '          if mode==2 or mode==3: \\n',\n",
       " '              self.Transformation_Class.phi.train()\\n',\n",
       " '              for param in self.Transformation_Class.phi.parameters():\\n',\n",
       " '                  param.requires_grad = True  # This unfreezes the weights\\n',\n",
       " '              accuracy = total_correct / total_samples\\n',\n",
       " '              return accuracy']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matcher = difflib.Differ()\n",
    "list(matcher.compare(a=A.splitlines(keepends=True), b=B.splitlines(keepends=True)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12f80f52-52fb-4398-b556-dbbab3a7652e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoConfig\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import torch.nn.init as init\n",
    "import math\n",
    "import nn\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdc73b61-d4ea-4d6b-9f9b-008d00b22dbd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[38;5;241m.\u001b[39mlm_head\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m=\u001b[39mnn\u001b[38;5;241m.\u001b[39mParameter(model\u001b[38;5;241m.\u001b[39mlm_head\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mclone())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "model.lm_head.weight=nn.Parameter(model.lm_head.weight.clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789486e0-163d-4570-b1c9-8f61c373caf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.embed_tokens.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c9af07-6990-4e37-9420-e97f3d15aa19",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.lm_head.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f224c1-9a63-4d24-b2b0-00c4d4dd40d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "init.kaiming_uniform_(model.lm_head.weight, a=math.sqrt(5)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fa06b1-102b-4ec2-9738-0b9b4f05ef98",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.embed_tokens.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cd7326-667b-4848-954c-33e800bf76f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.lm_head.weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153e4475-850f-40f4-8644-250e1ff0fdad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a099aefc-46a6-454f-8af6-8115954381b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Dataset_Generation import (make_intervention_dataset_AndOr,\n",
    "                                make_intervention_dataset_AndOrAnd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b14cbe0a-2905-43ca-8f43-3e1c22678ded",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'base': tensor([ 0.3063, -0.2930,  0.3063, -0.2930,  0.3972, -0.2602,  0.3972, -0.2602,\n",
       "           0.1138,  0.4179,  0.1138,  0.4179]),\n",
       "  'label': tensor(0.),\n",
       "  'sources': tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [-0.1240,  0.1019, -0.0826, -0.4761, -0.1284,  0.3261, -0.3001, -0.3660,\n",
       "            0.0652, -0.4335,  0.2481,  0.0448]]),\n",
       "  'intervention': [False, True]},\n",
       " {'base': tensor([-0.0928, -0.3593,  0.2855,  0.1725,  0.1946,  0.1924,  0.1946,  0.1924,\n",
       "           0.2857,  0.2265,  0.2857,  0.2265]),\n",
       "  'label': tensor(1.),\n",
       "  'sources': tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.1011, -0.4032, -0.4765,  0.3263, -0.0787, -0.2052, -0.0787, -0.2052,\n",
       "           -0.2033, -0.0161, -0.2033, -0.0161]]),\n",
       "  'intervention': [False, True]},\n",
       " {'base': tensor([-0.4111,  0.0273,  0.2595, -0.4515, -0.4495,  0.0763, -0.4080, -0.0547,\n",
       "           0.1087,  0.1786,  0.1087,  0.1786]),\n",
       "  'label': tensor(1.),\n",
       "  'sources': tensor([[ 0.3537,  0.4170,  0.3537,  0.4170,  0.0495, -0.1842,  0.0495, -0.1842,\n",
       "            0.4973, -0.2510,  0.4973, -0.2510],\n",
       "          [ 0.0066, -0.4357,  0.0066, -0.4357,  0.0505, -0.1708,  0.0505, -0.1708,\n",
       "           -0.3668,  0.4925, -0.3668,  0.4925]]),\n",
       "  'intervention': [True, True]},\n",
       " {'base': tensor([-0.1652, -0.4292, -0.1652, -0.4292, -0.3210,  0.2146,  0.3653, -0.1288,\n",
       "           0.1395,  0.2564, -0.2577,  0.3710]),\n",
       "  'label': tensor(0.),\n",
       "  'sources': tensor([[-0.2874,  0.2490,  0.2057,  0.2091,  0.3474, -0.2610,  0.3205, -0.4235,\n",
       "           -0.2473, -0.3629, -0.2473, -0.3629],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000]]),\n",
       "  'intervention': [True, False]},\n",
       " {'base': tensor([-0.0012,  0.0669, -0.1399,  0.4943, -0.1571,  0.2155, -0.1571,  0.2155,\n",
       "          -0.3817,  0.3740, -0.0693, -0.2524]),\n",
       "  'label': tensor(0.),\n",
       "  'sources': tensor([[-0.3469, -0.3880,  0.1105, -0.2644,  0.4954, -0.2271,  0.4954, -0.2271,\n",
       "            0.0148, -0.4063, -0.2905,  0.4412],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000]]),\n",
       "  'intervention': [True, False]},\n",
       " {'base': tensor([-0.1472, -0.3827, -0.0756, -0.0010,  0.4180,  0.4237,  0.4180,  0.4237,\n",
       "           0.1196, -0.3256,  0.1196, -0.3256]),\n",
       "  'label': tensor(0.),\n",
       "  'sources': tensor([[ 0.2022,  0.1145,  0.1743, -0.4521, -0.2534, -0.1293,  0.2193,  0.1122,\n",
       "            0.2874,  0.3637, -0.4213, -0.0339],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000]]),\n",
       "  'intervention': [True, False]},\n",
       " {'base': tensor([-0.3135, -0.0594,  0.4392, -0.3794,  0.3915,  0.1448,  0.0773, -0.2631,\n",
       "          -0.0784,  0.2726, -0.0784,  0.2726]),\n",
       "  'label': tensor(0.),\n",
       "  'sources': tensor([[-0.1878,  0.4209, -0.1878,  0.4209, -0.1719, -0.0521,  0.3124, -0.3456,\n",
       "            0.2115,  0.4686,  0.1918,  0.4318],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000]]),\n",
       "  'intervention': [True, False]},\n",
       " {'base': tensor([ 0.3880, -0.3660,  0.0999, -0.2832, -0.2585,  0.0850, -0.2585,  0.0850,\n",
       "          -0.1650,  0.2496, -0.1650,  0.2496]),\n",
       "  'label': tensor(0.),\n",
       "  'sources': tensor([[-0.3781, -0.0981, -0.3781, -0.0981,  0.4495, -0.0920, -0.0011,  0.0155,\n",
       "            0.1497, -0.4586,  0.0574,  0.3414],\n",
       "          [-0.3202,  0.3997, -0.3202,  0.3997, -0.0437,  0.4709, -0.0437,  0.4709,\n",
       "           -0.2401, -0.3725, -0.2401, -0.3725]]),\n",
       "  'intervention': [True, True]},\n",
       " {'base': tensor([-0.2691,  0.3812, -0.2143,  0.4828, -0.2538, -0.0371, -0.0970,  0.1424,\n",
       "           0.1438,  0.4589,  0.1438,  0.4589]),\n",
       "  'label': tensor(1.),\n",
       "  'sources': tensor([[ 0.4517,  0.4418,  0.3001, -0.4928,  0.4749, -0.4208,  0.4749, -0.4208,\n",
       "           -0.0341, -0.1227, -0.0341, -0.1227],\n",
       "          [ 0.0766, -0.1865, -0.4155, -0.4437,  0.1658, -0.2085,  0.1658, -0.2085,\n",
       "           -0.0139,  0.3144, -0.0139,  0.3144]]),\n",
       "  'intervention': [True, True]},\n",
       " {'base': tensor([ 0.4096, -0.1047,  0.3514, -0.3925, -0.0923, -0.1797, -0.0923, -0.1797,\n",
       "           0.4275,  0.4631,  0.4275,  0.4631]),\n",
       "  'label': tensor(0.),\n",
       "  'sources': tensor([[ 0.0798,  0.3591,  0.0798,  0.3591,  0.2687,  0.4031,  0.3174, -0.2862,\n",
       "           -0.1885, -0.4063,  0.1208,  0.0409],\n",
       "          [-0.1437, -0.2243,  0.4924,  0.2972,  0.1684, -0.3292,  0.1662,  0.0219,\n",
       "            0.0796, -0.1257,  0.0796, -0.1257]]),\n",
       "  'intervention': [True, True]}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_intervention_dataset_AndOr(10,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4695ab42-0cd0-4048-8b0a-0c048c245a73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 5]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(4,6))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
